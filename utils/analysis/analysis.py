"""
utils/analysis/analysis.py
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Herramientas de post‚Äëan√°lisis para modelos Keras. 100% compatibles con:

‚Ä¢  tf.data.Dataset  ‚Üí  (X, y)  o  (X, y, idx)
‚Ä¢  Tuplas NumPy     ‚Üí  (X_val, y_val)

Incluye funci√≥n para recuperar los √≠ndices originales de las muestras
mal clasificadas cuando el √≠ndice se haya a√±adido en el Dataset.
"""

from __future__ import annotations

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import (
    confusion_matrix,
    ConfusionMatrixDisplay,
    classification_report,
)


class ExperimentAnalyzer:
    # ------------------------------------------------------------------ #
    #  CONSTRUCTOR
    # ------------------------------------------------------------------ #
    def __init__(
        self,
        model,
        history,
        val_data,
        class_names: list[str] | None = None,
        effects: np.ndarray | None = None,
    ):
        """
        Parameters
        ----------
        model        : tf.keras.Model ya entrenado.
        history      : Objeto retornado por `model.fit()` o bien `history.history`.
        val_data     : ‚Ä¢ tf.data.Dataset -> (X, y)  o  (X, y, idx)
                       ‚Ä¢ tupla (X_val, y_val)  (NumPy)  *sin √≠ndice*.
        class_names  : Lista opcional con nombres legibles de las clases.
        """
        self.model = model
        self.history = history.history if hasattr(history, "history") else history
        self.class_names = class_names
        self.effects = effects

        # Convierte val_data a arrays (X, y, idx | None)
        self.X_val, self.y_val, self.idx_val = self._dataset_to_numpy(val_data)


    # ------------------------------------------------------------------ #
    #  M√âTODOS P√öBLICOS
    # ------------------------------------------------------------------ #
    def plot_training_curves(self) -> None:
        """Gr√°fica de p√©rdida y exactitud (train / val)."""
        epochs = range(1, len(self.history["loss"]) + 1)

        plt.figure(figsize=(12, 4))

        # ‚Äî Loss ‚Äî
        plt.subplot(1, 2, 1)
        plt.plot(epochs, self.history["loss"], label="Train Loss")
        plt.plot(epochs, self.history["val_loss"], label="Val Loss")
        plt.title("Loss por √âpoca")
        plt.xlabel("√âpoca"); plt.ylabel("Loss"); plt.legend()

        # ‚Äî Accuracy ‚Äî
        plt.subplot(1, 2, 2)
        plt.plot(epochs, self.history["accuracy"], label="Train Acc")
        plt.plot(epochs, self.history["val_accuracy"], label="Val Acc")
        plt.title("Accuracy por √âpoca")
        plt.xlabel("√âpoca"); plt.ylabel("Accuracy"); plt.legend()

        plt.tight_layout(); plt.show()

    # ------------------------------------------------------------------ #
    def confusion_matrix(self, normalize: str | None = None):
        """
        Dibuja la matriz de confusi√≥n.

        normalize : 'true', 'pred', 'all' o None.
        Siempre fuerza el n√∫mero de etiquetas a coincidir con `class_names`
        (si se proporcionan) para evitar desajustes de ticks.
        """
        y_pred = self._predict_classes(self.X_val)

        # ‚Äî Selecci√≥n de labels ‚Äî
        if self.class_names is not None:
            n_labels = len(self.class_names)
            labels = list(range(n_labels))
        else:
            labels = np.unique(np.concatenate([self.y_val, y_pred])).tolist()
        
        cm = confusion_matrix(self.y_val, y_pred,
                              labels=labels, normalize=normalize)

        disp = ConfusionMatrixDisplay(cm,
                                      display_labels=self.class_names or labels)
        fig, ax = plt.subplots(figsize=(7, 7))
        disp.plot(ax=ax, cmap="Blues", xticks_rotation=90, colorbar=False)

        norm_txt = f" (normalizada={normalize})" if normalize else ""
        ax.set_title(f"Matriz de Confusi√≥n{norm_txt}")
        plt.show()

    # ------------------------------------------------------------------ #
    def classification_report(self) -> None:
        """Imprime precisi√≥n, recall y F1 por clase."""
        y_pred = self._predict_classes(self.X_val)
        report = classification_report(
            self.y_val,
            y_pred,
            target_names=self.class_names,
            digits=4,
            zero_division=0,
        )
        print("\nüìÑ Classification Report\n")
        print(report)

    # ------------------------------------------------------------------ #
    def misclassified_indices(self) -> list[int]:
        """
        Retorna los √≠ndices originales (en el HDF5) de las muestras cuya
        predicci√≥n es incorrecta.  Requiere que `val_data` incluya √≠ndices.
        """
        if self.idx_val is None:
            raise ValueError(
                "El Dataset de validaci√≥n no incluye √≠ndices. "
                "Inicializa tu Dataset con `include_index=True`."
            )
        y_pred = self._predict_classes(self.X_val)
        mask = y_pred != self.y_val
        return self.idx_val[mask].tolist()

    # ------------------------------------------------------------------ #
    #  M√âTODOS PRIVADOS / HELPERS
    # ------------------------------------------------------------------ #
    @staticmethod
    def _dataset_to_numpy(val_data):
        """
        Convierte:
          ‚Ä¢ tf.data.Dataset ‚Üí concatena lotes en arrays NumPy.
          ‚Ä¢ (X_val, y_val)  ‚Üí Devuelve tal cual + idx=None.
        Devuelve (X, y, idx_or_None).
        """
        if isinstance(val_data, tuple):
            X, y = val_data
            return X, y, None

        xs, ys, idxs = [], [], []
        for batch in val_data:
            # Permite batches (X, y)   o   (X, y, idx)
            if len(batch) == 3:
                x, y, idx = batch
                idxs.append(idx.numpy())
            else:
                x, y = batch
            xs.append(x.numpy());  ys.append(y.numpy())
        X = np.concatenate(xs)
        Y = np.concatenate(ys)
        IDX = np.concatenate(idxs) if idxs else None
        return X, Y, IDX

    # ------------------------------------------------------------------ #
    def _predict_classes(self, X, batch_size: int = 512):
        """Aplica el modelo y devuelve argmax sobre el eje de clases."""
        probs = self.model.predict(X, batch_size=batch_size, verbose=0)
        return np.argmax(probs, axis=-1)


    # ------------------------------------------------------------------ #
    def effect_diagnostics(self, field: str):
        """
        Visualiza c√≥mo un *effect* influye en los fallos de clasificaci√≥n.
        field debe ser una columna de self.effects
        """
        if self.effects is None:
            raise ValueError("No se pas√≥ el structured array 'effects'.")

        if field not in self.effects.dtype.names:
            raise ValueError(f"'{field}' no existe en Effects.")

        y_pred = self._predict_classes(self.X_val)
        correct = y_pred == self.y_val
        df = pd.DataFrame({
            field: self.effects[field],
            "correct": correct
        })

        # Categ√≥rico si ‚â§10 valores √∫nicos; continuo en caso contrario
        if df[field].dtype.kind in "iu" and df[field].nunique() <= 10:
            plt.figure(figsize=(6,4))
            sns.countplot(x=field, hue="correct", data=df, palette="Set2")
            plt.title(f"Errores vs {field}")
            plt.ylabel("n¬∫ de se√±ales"); plt.show()
        else:  # continuo
            plt.figure(figsize=(6,4))
            sns.histplot(data=df, x=field, hue="correct",
                         bins=20, element="step", stat="density", common_norm=False)
            plt.title(f"Distribuci√≥n de {field} (correct / fail)")
            plt.show()